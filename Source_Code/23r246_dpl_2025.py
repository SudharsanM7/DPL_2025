# -*- coding: utf-8 -*-
"""23R246 DPL 2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19TIvQq8XI2NLnHiWLVdnt4Rzjc3hmBXX
"""

!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1
!python3 --version

!pip install pandas pyarrow numpy pyaml scikit-learn lightgbm networkx matplotlib joblib tqdm
!pip install -q pyaml

from google.colab import drive
drive.mount('/content/drive')
DATA_ROOT = "/content/drive/MyDrive/DPL 2025"
OUT_ROOT = "/content/drive/MyDrive/DPL 2025/artifacts"
import os
os.makedirs(OUT_ROOT, exist_ok=True)
print("DATA_ROOT:", DATA_ROOT)
print("OUT_ROOT:", OUT_ROOT)

import os, sys, json, math, warnings
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm
import joblib
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error
import lightgbm as lgb
warnings.simplefilter("ignore")
YEARS = dict(start=2000, end=2024)
ISO_MAP = {
  "India":"IND","USA":"USA","Russia":"RUS","France":"FRA","Germany":"DEU","Italy":"ITA",
  "China":"CHN","Japan":"JPN","Argentina":"ARG","Portugal":"PRT","Spain":"ESP","Croatia":"HRV",
  "Belgium":"BEL","Australia":"AUS","Pakistan":"PAK","Afghanistan":"AFG","Israel":"ISR",
  "Iran":"IRN","Iraq":"IRQ","Bangladesh":"BGD","Sri Lanka":"LKA","Canada":"CAN","UK":"GBR",
  "Sweden":"SWE","Saudi Arabia":"SAU"
}

WDI_SERIES_MAP = {
  "NE.IMP.GNFS.ZS": "imports_goods_services_gdp_pct",
  "NE.EXP.GNFS.ZS": "exports_goods_services_gdp_pct",
  "NE.TRD.GNFS.ZS": "trade_gdp_pct",
  "FP.CPI.TOTL.ZG": "inflation_consumer_prices_pct",
  "NY.GDP.MKTP.KD.ZG": "gdp_growth_pct",
  "NY.GDP.PCAP.CD": "gdp_per_capita_current_usd",
  "NY.GDP.MKTP.CD": "gdp_current_usd"
}
WELFARE_SERIES_MAP = {
  "SP.URB.TOTL.IN.ZS": "urban_pop_pct",
  "SP.DYN.LE00.IN": "life_expectancy_total_years",
  "SP.DYN.LE00.FE.IN": "life_expectancy_female_years",
  "SL.UEM.TOTL.ZS": "unemployment_total_pct",
}

TRADE_VALUE_PRIORITY = ["primaryValue","fobvalue","cifvalue"]  # try in order
TRADE_VOL_FIELD = "netWgt"
TRADE_REPORTER_FIELD = "reporterISO"
TRADE_PARTNER_FIELD_CANDIDATES = ["partnerISO","partnerISO3","partnerCountryISO","partner_iso"]
TRADE_YEAR_FIELD = "refYear"

def read_csv_smart(path, enc_primary="utf-8", enc_fallback="latin-1"):
    try:
        return pd.read_csv(path, encoding=enc_primary, low_memory=False)
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding=enc_fallback, low_memory=False)
    except FileNotFoundError:
        raise

def ensure_dir(d):
    os.makedirs(d, exist_ok=True)

ensure_dir(OUT_ROOT)

def melt_wdi_wide(df: pd.DataFrame, series_map: dict):
    df = df[df["Series Code"].isin(series_map.keys())].copy()
    year_cols = [c for c in df.columns if len(c) >= 4 and c[:4].isdigit()]
    if len(year_cols) == 0:
        raise ValueError("No year columns found in WDI-like file")
    id_vars = ["Country Name","Country Code","Series Name","Series Code"]
    m = df.melt(id_vars=id_vars, value_vars=year_cols, var_name="year_raw", value_name="value")
    m["year"] = m["year_raw"].str.slice(0,4).astype(int)
    m = m.rename(columns={"Country Name":"country", "Country Code":"iso3"})
    m["metric"] = m["Series Code"].map(series_map)
    m = m.dropna(subset=["metric"])
    m["value"] = pd.to_numeric(m["value"], errors="coerce")
    wide = m.pivot_table(index=["iso3","country","year"], columns="metric", values="value", aggfunc="first").reset_index()
    wide.columns.name = None
    return wide

wdi_files = {
    "core": os.path.join(DATA_ROOT, "Core_economic_indicators.csv"),
    "welfare": os.path.join(DATA_ROOT, "Social_and_welfare.csv"),
    "resilience": os.path.join(DATA_ROOT, "Resiliance.csv"),
    "employment": os.path.join(DATA_ROOT, "Employment_Unemployment.csv")
}

wdi_data = {}
for k, p in wdi_files.items():
    if os.path.exists(p):
        print("Reading:", p)
        df = read_csv_smart(p)
        wdi_data[k] = df
        print(f" - {k} shape:", df.shape)
    else:
        print("WARNING: WDI-style file not found:", p)

if "core" in wdi_data:
    econ_core = melt_wdi_wide(wdi_data["core"], WDI_SERIES_MAP)
    econ_core.to_parquet(os.path.join(OUT_ROOT, "econ_core.parquet"), index=False)
    print("Saved econ_core.parquet", econ_core.shape)

if "welfare" in wdi_data:
    welfare_df = melt_wdi_wide(wdi_data["welfare"], WELFARE_SERIES_MAP)
    welfare_df.to_parquet(os.path.join(OUT_ROOT, "welfare.parquet"), index=False)
    print("Saved welfare.parquet", welfare_df.shape)

def detect_partner_field(df):
    for cand in TRADE_PARTNER_FIELD_CANDIDATES:
        if cand in df.columns:
            return cand
    for c in df.columns:
        if "partner" in c.lower() and "iso" in c.lower():
            return c
    return None

def load_and_aggregate_trade(file_paths, flow_filter=None, restrict_reporters_iso=None):
    frames = []
    for p in file_paths:
        if not os.path.exists(p):
            print("Trade file missing:", p)
            continue
        df = read_csv_smart(p)
        frames.append(df)
    if len(frames) == 0:
        return None, None
    trade = pd.concat(frames, ignore_index=True, sort=False)
    reporter_field = TRADE_REPORTER_FIELD if TRADE_REPORTER_FIELD in trade.columns else "reporterCode"
    partner_field = detect_partner_field(trade)
    year_field = TRADE_YEAR_FIELD if TRADE_YEAR_FIELD in trade.columns else "refYear"
    if flow_filter is not None and "flowCode" in trade.columns:
        trade = trade[trade["flowCode"] == flow_filter]
    if restrict_reporters_iso is not None and reporter_field in trade.columns:
        trade = trade[trade[reporter_field].isin(restrict_reporters_iso)]
    def choose_value(row):
        for c in TRADE_VALUE_PRIORITY:
            if c in row.index and pd.notna(row[c]):
                return row[c]
        return pd.NA
    trade["value_usd"] = trade.apply(choose_value, axis=1)
    trade["value_usd"] = pd.to_numeric(trade["value_usd"], errors="coerce")
    trade["volume_tonnes"] = pd.to_numeric(trade[TRADE_VOL_FIELD] if TRADE_VOL_FIELD in trade.columns else pd.Series([np.nan]*len(trade)), errors="coerce")
    trade = trade.rename(columns={reporter_field: "iso3", year_field: "year"})
    if partner_field:
        trade = trade.rename(columns={partner_field: "partner_iso"})
    annual = trade.groupby(["iso3","year"], dropna=False).agg(
        trade_value_usd = ("value_usd","sum"),
        trade_volume_tonnes = ("volume_tonnes","sum"),
        records = ("value_usd","count")
    ).reset_index()
    if "partner_iso" in trade.columns:
        by_partner = trade.groupby(["iso3","year","partner_iso"], dropna=False).agg(partner_value_usd=("value_usd","sum")).reset_index()
        by_partner["share"] = by_partner.groupby(["iso3","year"])["partner_value_usd"].transform(lambda s: s / s.sum())
        hhi = by_partner.groupby(["iso3","year"])["share"].apply(lambda s: (s.fillna(0)**2).sum()).reset_index(name="trade_partner_hhi")
    else:
        hhi = pd.DataFrame(columns=["iso3","year","trade_partner_hhi"])
    return annual, hhi

imports_paths = [os.path.join(DATA_ROOT, "2000-2012_Import.csv"), os.path.join(DATA_ROOT, "2013-2024_Import.csv")]
exports_paths = [os.path.join(DATA_ROOT, "2000-2012_Export.csv"), os.path.join(DATA_ROOT, "2013-2024_Export.csv")]

restricters = list(ISO_MAP.values())

imports_annual, imports_hhi = load_and_aggregate_trade(imports_paths, flow_filter="M", restrict_reporters_iso=restricters)
exports_annual, exports_hhi = load_and_aggregate_trade(exports_paths, flow_filter="X", restrict_reporters_iso=restricters)

if imports_annual is not None:
    imports_annual.to_parquet(os.path.join(OUT_ROOT, "imports_annual.parquet"), index=False)
    print("Saved imports_annual", imports_annual.shape)
if exports_annual is not None:
    exports_annual.to_parquet(os.path.join(OUT_ROOT, "exports_annual.parquet"), index=False)
    print("Saved exports_annual", exports_annual.shape)

if imports_annual is not None and exports_annual is not None:
    trade_annual = imports_annual.merge(exports_annual, on=["iso3","year"], how="outer", suffixes=("_imp","_exp"))
    if (not imports_hhi.empty) or (not exports_hhi.empty):
        hhi_all = imports_hhi.merge(exports_hhi, on=["iso3","year"], how="outer", suffixes=("_imp","_exp"))
        def avg_hhi(r):
            a = r.get("trade_partner_hhi_imp")
            b = r.get("trade_partner_hhi_exp")
            if pd.notna(a) and pd.notna(b): return 0.5*(a+b)
            return a if pd.notna(a) else b
        hhi_all["trade_partner_hhi"] = hhi_all.apply(avg_hhi, axis=1)
        hhi_all = hhi_all[["iso3","year","trade_partner_hhi"]]
        trade_annual = trade_annual.merge(hhi_all, on=["iso3","year"], how="left")
    trade_annual.to_parquet(os.path.join(OUT_ROOT, "trade_annual.parquet"), index=False)
    print("Saved trade_annual", trade_annual.shape)

def load_population_fao(path, iso_map):
    df = read_csv_smart(path)
    total_code = 511
    item_code = 3010
    if "Element Code" in df.columns and "Item Code" in df.columns:
        keep = df[(df["Element Code"]==total_code) & (df["Item Code"]==item_code)].copy()
    else:
        raise ValueError("population file not in expected shape (missing Element Code / Item Code)")
    keep["year"] = keep["Year"].astype(int)
    keep["country"] = keep["Area"]
    reverse_iso = {v:k for k,v in iso_map.items()}
    def map_iso(area):
        if isinstance(area,str) and len(area)==3 and area.isupper():
            return area
        for k,v in iso_map.items():
            if area.strip().lower() == k.strip().lower():
                return v
        return None
    keep["iso3"] = keep["country"].apply(map_iso)

    unit_factor = 1
    if "Unit" in keep.columns:

        sample_unit = str(keep["Unit"].iloc[0]) if len(keep)>0 else ""
        if "1000" in sample_unit:
            unit_factor = 1000
    keep["population_total"] = pd.to_numeric(keep["Value"], errors="coerce") * unit_factor
    pop = keep[["iso3","country","year","population_total"]].dropna(subset=["iso3"])
    return pop

def load_disasters_emdat(path, iso_map, years_range, missing_as_zero=True):
    df = read_csv_smart(path)

    iso_field = "ISO" if "ISO" in df.columns else "iso"
    year_field = "Year" if "Year" in df.columns else None
    damage_field_candidates = [c for c in df.columns if "Total Damage" in c and "Adjusted" in c]
    damage_field = damage_field_candidates[0] if len(damage_field_candidates)>0 else None
    deaths_field_candidates = [c for c in df.columns if "Total Deaths" in c or "Total deaths" in c]
    deaths_field = deaths_field_candidates[0] if len(deaths_field_candidates)>0 else None
    affected_field_candidates = [c for c in df.columns if "Total Affected" in c or "Total affected" in c]
    affected_field = affected_field_candidates[0] if len(affected_field_candidates)>0 else None

    if year_field is None:

        if "DisNo." in df.columns:
            df["year"] = df["DisNo."].astype(str).str.slice(0,4).astype(int)
        else:
            raise ValueError("No year field in disasters file and no DisNo. to parse.")
    else:
        df["year"] = pd.to_numeric(df[year_field], errors="coerce").astype("Int64")

    df["iso3"] = df[iso_field]

    if damage_field:
        df["damage_usd"] = pd.to_numeric(df[damage_field], errors="coerce") * 1000.0
    else:
        df["damage_usd"] = 0.0
    if deaths_field:
        df["deaths"] = pd.to_numeric(df[deaths_field], errors="coerce")
    else:
        df["deaths"] = 0.0
    if affected_field:
        df["affected"] = pd.to_numeric(df[affected_field], errors="coerce")
    else:
        df["affected"] = 0.0

    agg = df.groupby(["iso3","year"], dropna=False).agg(
        disasters_events=("iso3","count"),
        disasters_damage_usd=("damage_usd","sum"),
        disasters_deaths=("deaths","sum"),
        disasters_affected=("affected","sum")
    ).reset_index()

    isos = list(ISO_MAP.values())
    full = []
    for iso in isos:
        for y in range(YEARS["start"], YEARS["end"]+1):
            full.append({"iso3": iso, "year": y})
    full_df = pd.DataFrame(full)
    merged = full_df.merge(agg, on=["iso3","year"], how="left")
    if missing_as_zero:
        merged[["disasters_events","disasters_damage_usd","disasters_deaths","disasters_affected"]] = merged[["disasters_events","disasters_damage_usd","disasters_deaths","disasters_affected"]].fillna(0.0)
    return merged

pop_path = os.path.join(DATA_ROOT, "population_and_demographics.csv")
disaster_path = os.path.join(DATA_ROOT, "disasters.csv")

if os.path.exists(pop_path):
    pop_df = load_population_fao(pop_path, ISO_MAP)
    pop_df.to_parquet(os.path.join(OUT_ROOT, "population.parquet"), index=False)
    print("Saved population.parquet", pop_df.shape)
else:
    print("WARNING: population file missing:", pop_path)

if os.path.exists(disaster_path):
    disasters_df = load_disasters_emdat(disaster_path, ISO_MAP, YEARS, missing_as_zero=True)
    disasters_df.to_parquet(os.path.join(OUT_ROOT, "disasters.parquet"), index=False)
    print("Saved disasters.parquet", disasters_df.shape)
else:
    print("WARNING: disasters file missing:", disaster_path)

parts = {}
for name in ["econ_core","welfare","resilience","imports_annual","exports_annual","trade_annual","population","disasters"]:
    path = os.path.join(OUT_ROOT, f"{name}.parquet")
    if os.path.exists(path):
        try:
            parts[name] = pd.read_parquet(path)
            print("Loaded:", name, parts[name].shape)
        except Exception as e:
            print("Could not load", name, path, e)

econ = parts.get("econ_core", None)
welfare = parts.get("welfare", None)
trade = parts.get("trade_annual", None)
population = parts.get("population", None)
disasters = parts.get("disasters", None)
employment = parts.get("employment", None) if os.path.exists(os.path.join(OUT_ROOT, "employment.parquet")) else None

rows = []
revmap = {v:k for k,v in ISO_MAP.items()}
for iso in ISO_MAP.values():
    for y in range(YEARS["start"], YEARS["end"]+1):
        rows.append({"iso3": iso, "year": y, "country": revmap.get(iso, iso)})
master = pd.DataFrame(rows)

joins = {
    "econ_core": econ,
    "welfare": welfare,
    "trade_annual": trade,
    "population": population,
    "disasters": disasters,
    "employment": employment
}
for k, df in joins.items():
    if df is None:
        print("Skipping join — missing:", k)
        continue
    if "iso3" not in df.columns and "iso" in df.columns:
        df = df.rename(columns={"iso": "iso3"})
    master = master.merge(df, on=["iso3","year"], how="left", suffixes=("","_"+k))

print("Master shape after merges:", master.shape)

num_cols = master.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [c for c in num_cols if c not in ("year",)]
def interp_group(g):
    g = g.sort_values("year")
    g[num_cols] = g[num_cols].interpolate(method="linear", limit=2, limit_direction="both")
    return g

master = master.groupby("iso3", group_keys=False).apply(interp_group)

for c in master.columns:
    if c.endswith("_index") or c.endswith("_score"):
        master[c] = master.groupby("iso3")[c].ffill(limit=3).bfill(limit=3)

alpha = 0.5
if "trade_gdp_pct" in master.columns:
    master["trade_partner_hhi"] = master.get("trade_partner_hhi", np.nan)
    master["trade_dependency_index"] = master["trade_gdp_pct"] * (1.0 + alpha * master["trade_partner_hhi"].clip(lower=0, upper=1).fillna(0))
else:
    print("WARNING: trade_gdp_pct not present — cannot compute trade_dependency_index")

master_path = os.path.join(OUT_ROOT, "tren_master.parquet")
master.to_parquet(master_path, index=False)
print("Saved master panel:", master_path, master.shape)

import math
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer

def compute_resilience_score(df):
    candidates = []
    if "trade_partner_hhi" in df.columns:
        candidates.append(("trade_partner_hhi", True))
    if "disasters_damage_usd" in df.columns:
        candidates.append(("disasters_damage_usd", False))
    if "unemployment_total_pct" in df.columns:
        candidates.append(("unemployment_total_pct", False))
    if "external_debt_pct" in df.columns:
        candidates.append(("external_debt_pct", False))
    if "gdp_per_capita_current_usd" in df.columns:
        candidates.append(("gdp_per_capita_current_usd", True))
    if "life_expectancy_total_years" in df.columns:
        candidates.append(("life_expectancy_total_years", True))
    if "urban_pop_pct" in df.columns:
        candidates.append(("urban_pop_pct", True))

    checked = [(c,pos) for c,pos in candidates if c in df.columns]

    if len(checked) == 0:
        fallback = []
        if "gdp_per_capita_current_usd" in df.columns:
            fallback.append(("gdp_per_capita_current_usd", True))
        if "trade_dependency_index" in df.columns:
            fallback.append(("trade_dependency_index", False))
        checked = fallback

    if len(checked) == 0:
        df["resilience_score"] = np.nan
        return df

    mat = df[[c for c,_ in checked]].copy()

    for c,pos in checked:
        if pos is False:
            mat[c] = -mat[c]

    imputer = SimpleImputer(strategy="median")
    mat_imputed = imputer.fit_transform(mat)

    scaler = StandardScaler()
    mat_s = scaler.fit_transform(mat_imputed)

    pca = PCA(n_components=1, random_state=42)
    pc1 = pca.fit_transform(mat_s).ravel()

    pc1_scaled = 50 + 10 * (pc1 - np.mean(pc1)) / (np.std(pc1) + 1e-9)
    pc1_min, pc1_max = pc1_scaled.min(), pc1_scaled.max()
    resilience = 100.0 * (pc1_scaled - pc1_min) / (pc1_max - pc1_min + 1e-9)

    df["resilience_score"] = resilience
    return df

def compute_shock_impact_score(df):

    if "disasters_damage_usd" not in df.columns or "population_total" not in df.columns:
        df["shock_impact_score"] = np.nan
        return df

    df["damage_per_capita"] = df["disasters_damage_usd"] / (df["population_total"].replace({0:np.nan}))

    if "gdp_growth_pct" in df.columns:
        df = df.sort_values(["iso3","year"])
        df["gdp_growth_next"] = df.groupby("iso3")["gdp_growth_pct"].shift(-1)
        df["gdp_growth_drop_next"] = df["gdp_growth_next"] - df["gdp_growth_pct"]
    else:
        df["gdp_growth_drop_next"] = 0.0

    def country_score(g):
        dmg = g["damage_per_capita"].fillna(0.0)
        thresh = np.percentile(dmg, 90) if len(dmg.dropna())>0 else np.nan
        severe = g[dmg >= thresh] if not math.isnan(thresh) else g.iloc[0:0]
        avg_drop = severe["gdp_growth_drop_next"].mean() if len(severe)>0 else 0.0
        score = -avg_drop + dmg.mean() * 1e-6
        return score

    country_scores = df.groupby("iso3").apply(country_score).reset_index(name="cscore")
    df = df.merge(country_scores, on="iso3", how="left")

    s = df["cscore"].fillna(0.0)
    smin, smax = s.min(), s.max()
    if smax - smin > 0:
        df["shock_impact_score"] = 100.0 * (s - smin) / (smax - smin)
    else:
        df["shock_impact_score"] = 0.0
    return df

def compute_spending_efficiency(df):

    spend_col_candidates = [c for c in df.columns if "gov" in c.lower() and "spend" in c.lower()]
    spend_col = spend_col_candidates[0] if len(spend_col_candidates)>0 else None
    welfare_col = "life_expectancy_total_years" if "life_expectancy_total_years" in df.columns else None
    df["spending_efficiency"] = np.nan
    if spend_col and welfare_col:
        df = df.sort_values(["iso3","year"])
        df["d_welfare"] = df.groupby("iso3")[welfare_col].diff()
        df["d_spend"] = df.groupby("iso3")[spend_col].diff()
        df["spending_efficiency"] = (df["d_welfare"] / (df["d_spend"].replace({0:np.nan}))).replace([np.inf,-np.inf], np.nan)
    return df

master = pd.read_parquet(os.path.join(OUT_ROOT, "tren_master.parquet"))
master_fe = compute_resilience_score(master.copy())
master_fe = compute_shock_impact_score(master_fe)
master_fe = compute_spending_efficiency(master_fe)


master_fe.to_parquet(os.path.join(OUT_ROOT, "tren_master_fe.parquet"), index=False)
print("Saved tren_master_fe.parquet", master_fe.shape)

df = pd.read_parquet(os.path.join(OUT_ROOT, "tren_master_fe.parquet"))
df = df.sort_values(["iso3","year"])
df["target_gdp_growth_next"] = df.groupby("iso3")["gdp_growth_pct"].shift(-1)

model_df = df[~df["target_gdp_growth_next"].isna()].copy()
print("Training rows:", model_df.shape)

def create_lags(g, cols, lags=[1,2,3]):
    g = g.sort_values("year")
    for c in cols:
        for l in lags:
            g[f"{c}_lag{l}"] = g[c].shift(l)
    return g

numeric_seed_cols = ["gdp_growth_pct","inflation_consumer_prices_pct","trade_gdp_pct","trade_dependency_index","trade_partner_hhi","disasters_damage_usd","unemployment_total_pct","gdp_per_capita_current_usd","resilience_score","shock_impact_score"]
existing_cols = [c for c in numeric_seed_cols if c in df.columns]
model_df = model_df.groupby("iso3", group_keys=False).apply(lambda g: create_lags(g, existing_cols, lags=[1,2,3]))

feature_cols = [c for c in model_df.columns if (("_lag" in c) or (c in ["trade_dependency_index","resilience_score","shock_impact_score","trade_partner_hhi"])) ]

model_df["year_int"] = model_df["year"].astype(int)
model_df["iso_id"] = model_df["iso3"].astype('category').cat.codes

model_df = model_df.dropna(subset=[fc for fc in feature_cols if fc is not None], how="all")
FEATURES = [c for c in feature_cols if c in model_df.columns] + ["year_int","iso_id"]
TARGET = "target_gdp_growth_next"
print("Features used:", FEATURES)

years_unique = sorted(model_df["year"].unique())
n_splits = min(5, max(2, len(years_unique)//3))
print("TimeSeriesSplit n_splits:", n_splits)
tscv = TimeSeriesSplit(n_splits=n_splits)

X = model_df[FEATURES]
y = model_df[TARGET]

params = {
    "objective": "regression",
    "metric": "rmse",
    "learning_rate": 0.05,
    "num_leaves": 31,
    "min_data_in_leaf": 20,
    "verbose": -1,
    "n_estimators": 2000
}

models = []
fold = 0
best_iterations = []
for train_index, val_index in tscv.split(X):
    fold += 1
    X_train, X_val = X.iloc[train_index], X.iloc[val_index]
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]
    dtrain = lgb.Dataset(X_train, y_train)
    dval = lgb.Dataset(X_val, y_val, reference=dtrain)
    model = lgb.train(params, dtrain, valid_sets=[dtrain,dval],
                      callbacks=[lgb.early_stopping(100, verbose=False)])
    models.append(model)
    best_iterations.append(model.best_iteration)
    print(f"Fold {fold} trained. Best iter: {model.best_iteration}")

dtrain_full = lgb.Dataset(X, y)
mean_best_iteration = int(np.mean(best_iterations)) if best_iterations else 1
final_num_boost_round = max(1, mean_best_iteration)
print(f"Mean best iteration from CV: {mean_best_iteration}, Final boosting rounds: {final_num_boost_round}")

final_model = lgb.train(params, dtrain_full, num_boost_round=final_num_boost_round)

q_models = {}
for alpha in [0.1, 0.9]:
    p = params.copy()
    p["objective"] = "quantile"
    p["alpha"] = alpha
    m = lgb.train(p, dtrain_full, num_boost_round=final_num_boost_round)
    q_models[alpha] = m

joblib.dump(final_model, os.path.join(OUT_ROOT, "lgb_gdp_growth_model.pkl"))
joblib.dump(q_models, os.path.join(OUT_ROOT, "lgb_gdp_growth_quantiles.pkl"))
print("Saved models.")

import copy

def forecast_iterative(df_master, model, qmodels, features, start_year=2025, end_year=2030, scenario=None):

    df = df_master.copy().sort_values(["iso3","year"]).reset_index(drop=True)
    isos = df["iso3"].unique().tolist()
    future_rows = []
    last_year = df["year"].max()
    for iso in isos:
        country = df[df["iso3"]==iso]["country"].dropna().unique()
        country = country[0] if len(country)>0 else iso
        for y in range(last_year+1, end_year+1):
            future_rows.append({"iso3": iso, "country": country, "year": y})
    future = pd.DataFrame(future_rows)
    work = pd.concat([df, future], ignore_index=True, sort=False)
    work = work.sort_values(["iso3","year"]).reset_index(drop=True)

    for f in features:
        if f not in work.columns:
            work[f] = np.nan

    def fill_exog(g, cols):
        g = g.sort_values("year")
        for c in cols:
            if g[c].notna().sum() >= 2:
                last = g[c].dropna().tail(5)
                if len(last) >= 2:
                    yrs = np.arange(len(last))
                    coef = np.polyfit(yrs, last.values, 1)
                    trend = coef[0]
                    for i, y in enumerate(range(g["year"].max()+1 - len(last) + 1, g["year"].max()+1)):
                        pass
                    g[c] = g[c].ffill()
                else:
                    g[c] = g[c].ffill()
            else:
                g[c] = g[c].fillna(method="ffill")
        return g

    for iso in isos:
        g = work[work["iso3"]==iso].sort_values("year")
        g = fill_exog(g, [c for c in features if not c.endswith("_lag1") and not "_lag" in c])
        for y in range(last_year+1, end_year+1):
            row_idx = g[g["year"]==y].index
            if len(row_idx)==0:
                continue
            row_i = row_idx[0]
            for base in [col.replace("_lag1","") for col in features if col.endswith("_lag1")]:
                prev = g.loc[g["year"]==y-1]
                if not prev.empty:
                    g.at[row_i, f"{base}_lag1"] = prev.iloc[0].get(base, np.nan)
                prev2 = g.loc[g["year"]==y-2]
                if not prev2.empty:
                    g.at[row_i, f"{base}_lag2"] = prev2.iloc[0].get(base, np.nan)
                prev3 = g.loc[g["year"]==y-3]
                if not prev3.empty:
                    g.at[row_i, f"{base}_lag3"] = prev3.iloc[0].get(base, np.nan)

            if scenario:
                if "trade_hhi_reduction" in scenario and "trade_partner_hhi" in g.columns:
                    g.at[row_i, "trade_partner_hhi"] = g.at[row_i, "trade_partner_hhi"] * (1.0 - scenario["trade_hhi_reduction"])
                if "social_spending_add_pp" in scenario and "government_spending_pct" in g.columns:
                    g.at[row_i, "government_spending_pct"] = g.at[row_i, "government_spending_pct"] + scenario["social_spending_add_pp"]

            feat_row = g.loc[row_i, features].values.reshape(1, -1)
            feat_row = np.nan_to_num(feat_row, nan=0.0)

            pred = model.predict(feat_row)[0]
            lower = qmodels[0.1].predict(feat_row)[0] if 0.1 in qmodels else pred
            upper = qmodels[0.9].predict(feat_row)[0] if 0.9 in qmodels else pred
            g.at[row_i, "gdp_growth_pct"] = pred
            g.at[row_i, "pred_gdp_growth_median"] = pred
            g.at[row_i, "pred_gdp_growth_lower"] = lower
            g.at[row_i, "pred_gdp_growth_upper"] = upper

        for c in g.columns:
            work.loc[g.index, c] = g[c]

    forecasts = work[work["year"]>=last_year+1].copy()
    return forecasts

features_for_forecast = FEATURES
forecast_baseline = forecast_iterative(master, final_model, q_models, features_for_forecast, start_year=2025, end_year=2030, scenario=None)
forecast_baseline.to_parquet(os.path.join(OUT_ROOT, "forecast_baseline_2025_2030.parquet"), index=False)
print("Saved baseline forecast:", forecast_baseline.shape)

from math import sqrt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import lightgbm as lgb
import pandas as pd
import numpy as np

def backtest_expanding(df_all, model_builder, features, target_col="target_gdp_growth_next", min_train_year=2005):
    years = sorted(df_all["year"].unique())
    test_years = [y for y in years if y >= min_train_year and y <= 2022]
    for test_y in test_years:
        train_df = df_all[df_all["year"] < test_y].dropna(subset=[target_col])
        test_df = df_all[df_all["year"] == test_y].dropna(subset=[target_col])
        if train_df.shape[0] < 100 or test_df.shape[0] == 0:
            continue
        X_train = train_df[features]; y_train = train_df[target_col]
        X_test = test_df[features]; y_test = test_df[target_col]
        mdl = model_builder(X_train, y_train)
        y_pred = mdl.predict(X_test)
        rmse = sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        records.append({"year": test_y, "rmse": rmse, "mae": mae, "n_test": len(y_test)})
        print(f"Backtest year {test_y} RMSE {rmse:.4f} MAE {mae:.4f} (n={len(y_test)})")
    return pd.DataFrame(records)

def build_lgb_model(X, y):
    dtrain = lgb.Dataset(X, y)
    params = {"objective":"regression","metric":"rmse","learning_rate":0.05,"num_leaves":31,"min_data_in_leaf":20}
    model = lgb.train(params, dtrain, num_boost_round=200)
    return model

if 'model_df' not in globals():
    try:
        df = pd.read_parquet(os.path.join(OUT_ROOT, "tren_master_fe.parquet"))
        df = df.sort_values(["iso3","year"])
        df["target_gdp_growth_next"] = df.groupby("iso3")["gdp_growth_pct"].shift(-1)
        model_df = df[~df["target_gdp_growth_next"].isna()].copy()

        def create_lags(g, cols, lags=[1,2,3]):
            g = g.sort_values("year")
            for c in cols:
                for l in lags:
                    g[f"{c}_lag{l}"] = g[c].shift(l)
            return g

        numeric_seed_cols = ["gdp_growth_pct","inflation_consumer_prices_pct","trade_gdp_pct","trade_dependency_index","trade_partner_hhi","disasters_damage_usd","unemployment_total_pct","gdp_per_capita_current_usd","resilience_score","shock_impact_score"]
        existing_cols = [c for c in numeric_seed_cols if c in df.columns]
        model_df = model_df.groupby("iso3", group_keys=False).apply(lambda g: create_lags(g, existing_cols, lags=[1,2,3]))

        feature_cols = [c for c in model_df.columns if (("_lag" in c) or (c in ["trade_dependency_index","resilience_score","shock_impact_score","trade_partner_hhi"])) ]
        model_df["year_int"] = model_df["year"].astype(int)
        model_df["iso_id"] = model_df["iso3"].astype('category').cat.codes
        model_df = model_df.dropna(subset=[fc for fc in feature_cols if fc is not None], how="all")
        FEATURES = [c for c in feature_cols if c in model_df.columns] + ["year_int","iso_id"]

    except FileNotFoundError:
        print("Error: tren_master_fe.parquet not found. Please run previous cells.")
        raise

backtest_df = model_df.copy()
results_bt = backtest_expanding(backtest_df, build_lgb_model, FEATURES)
results_bt.to_csv(os.path.join(OUT_ROOT, "backtest_results.csv"), index=False)
print("Saved backtest_results.csv")

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")

fc = pd.read_parquet(os.path.join(OUT_ROOT, "forecast_baseline_2025_2030.parquet"))

pivot = fc.pivot_table(index="iso3", columns="year", values="pred_gdp_growth_median", aggfunc="first")
plt.figure(figsize=(12,10))
sns.heatmap(pivot, annot=True, fmt=".2f", cmap="RdBu_r", center=2.0)
plt.title("Predicted GDP growth (%) 2025-2030 (median)")
plt.tight_layout()
plt.savefig(os.path.join(OUT_ROOT, "heatmap_gdp_growth_2025_2030.png"))
plt.show()

trade_df = pd.read_parquet(os.path.join(OUT_ROOT, "trade_annual.parquet"))

if "trade_partner_hhi" in trade_df.columns:
    print("trade_partner_hhi is available; plotting trade_dependency vs HHI")
    sample = trade_df[trade_df["year"]==2023].sort_values("trade_value_usd_imp", ascending=False)
    plt.figure(figsize=(10,5))
    plt.scatter(sample["trade_value_usd_imp"].fillna(0), sample["trade_partner_hhi"].fillna(0))
    plt.xlabel("Trade value (imports) USD")
    plt.ylabel("Partner HHI")
    plt.title("Imports value vs Partner HHI (2023)")
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_ROOT, "imports_hhi_scatter_2023.png"))
    plt.show()
else:
    print("Partner breakdown not available for detailed network plot.")

master_fe = pd.read_parquet(os.path.join(OUT_ROOT, "tren_master_fe.parquet"))
vulns = []
for iso in master_fe["iso3"].unique():
    sub = master_fe[master_fe["iso3"]==iso].sort_values("year").tail(5).mean(numeric_only=True)
    scores = {}
    scores["trade_dependency_index"] = sub.get("trade_dependency_index", np.nan)
    scores["shock_impact_score"] = sub.get("shock_impact_score", np.nan)
    scores["resilience_score"] = sub.get("resilience_score", np.nan)
    scores["avg_disaster_damage_percap"] = (sub.get("disasters_damage_usd", 0.0) / (sub.get("population_total", 1.0)))

    ranking = sorted(scores.items(), key=lambda x: (-x[1] if x[0]!="resilience_score" else x[1]))
    top3 = [k for k,_ in ranking[:3]]
    vulns.append({"iso3": iso, "country": revmap.get(iso, iso), "top3_vulnerabilities": top3})
vuln_df = pd.DataFrame(vulns)
vuln_df.to_csv(os.path.join(OUT_ROOT, "country_vulnerabilities.csv"), index=False)
print("Saved country_vulnerabilities.csv")

fc = pd.read_parquet(os.path.join(OUT_ROOT, "forecast_baseline_2025_2030.parquet"))
vuln = pd.read_csv(os.path.join(OUT_ROOT, "country_vulnerabilities.csv"))

reports = []
for iso in fc["iso3"].unique():
    sub = fc[fc["iso3"]==iso].sort_values("year")
    row2030 = sub[sub["year"]==2030]
    if len(row2030)==0:
        continue
    med = float(row2030["pred_gdp_growth_median"].iloc[0])
    low = float(row2030["pred_gdp_growth_lower"].iloc[0]) if "pred_gdp_growth_lower" in row2030.columns else med
    high = float(row2030["pred_gdp_growth_upper"].iloc[0]) if "pred_gdp_growth_upper" in row2030.columns else med
    top3 = vuln[vuln["iso3"]==iso]["top3_vulnerabilities"].iloc[0] if iso in vuln["iso3"].values else []
    suggestions = []
    if "trade_dependency_index" in top3:
        suggestions.append("Diversify trade partners and build domestic substitute capacity for key imports.")
    if "shock_impact_score" in top3:
        suggestions.append("Invest in disaster risk reduction and shock-absorbing fiscal mechanisms.")
    if "resilience_score" in top3:
        suggestions.append("Increase fiscal buffers and social safety nets to improve resilience.")
    reports.append({
        "iso3": iso,
        "country": revmap.get(iso, iso),
        "gdp_growth_2030_med": med,
        "gdp_growth_2030_low": low,
        "gdp_growth_2030_high": high,
        "top3_vulnerabilities": top3,
        "recommendations_short": suggestions
    })

reports_df = pd.DataFrame(reports)
reports_df.to_csv(os.path.join(OUT_ROOT, "policy_briefs_summary.csv"), index=False)
with open(os.path.join(OUT_ROOT, "policy_briefs_summary.json"), "w", encoding="utf-8") as f:
    json.dump(reports, f, indent=2)
print("Saved per-country policy brief skeletons.")

import os, json, re
from openai import OpenAI

os.environ["HF_TOKEN"] = "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"

client = OpenAI(
    base_url="https://router.huggingface.co/v1",
    api_key=os.environ["HF_TOKEN"],
)

# Paths
SUMMARY_FILE = "/content/drive/MyDrive/DPL 2025/artifacts/policy_briefs_summary.json"
OUT_FILE = "/content/drive/MyDrive/DPL 2025/artifacts/policy_brief_llm.json"


def clean_recommendations(raw_text):

    recs = []
    for line in raw_text.split("\n"):
        line = line.strip()
        if not line:
            continue

        line = re.sub(r"^[\-\*\d\.\)\s]+", "", line)
        line = line.replace("**", "").strip()
        if line:
            recs.append(line)

    seen = set()
    final_recs = []
    for r in recs:
        if r not in seen:
            seen.add(r)
            final_recs.append(r)
        if len(final_recs) == 3:
            break
    return final_recs


def generate_recommendations(country, iso3, gdp_med, gdp_low, gdp_high, vulnerabilities):

    prompt = f"""
You are advising the government of {country} ({iso3}).

Forecast for 2030:
- Median GDP growth: {gdp_med:.1f}%
- Range: [{gdp_low:.1f}%, {gdp_high:.1f}%]
- Key vulnerabilities: {", ".join(vulnerabilities)}

Provide exactly 3 concise, actionable policy recommendations tailored to these vulnerabilities.
Do not use markdown or numbering. Each recommendation must be a single clear sentence.
"""

    completion = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct:novita",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=300,
    )

    raw_text = completion.choices[0].message.content
    recs = clean_recommendations(raw_text)

    while len(recs) < 3:
        recs.append("No further recommendation generated.")
    return recs


with open(SUMMARY_FILE, "r") as f:
    summary = json.load(f)

results = []
for entry in summary:
    iso3 = entry["iso3"]
    country = entry.get("country", iso3)
    gdp_med = entry["gdp_growth_2030_med"]
    gdp_low = entry["gdp_growth_2030_low"]
    gdp_high = entry["gdp_growth_2030_high"]

    vulnerabilities = entry["top3_vulnerabilities"]
    if isinstance(vulnerabilities, str):
        vulnerabilities = re.findall(r"[A-Za-z0-9_]+", vulnerabilities)

    recs = generate_recommendations(country, iso3, gdp_med, gdp_low, gdp_high, vulnerabilities)

    results.append({
        "iso3": iso3,
        "country": country,
        "gdp_growth_2030_med": gdp_med,
        "gdp_growth_2030_low": gdp_low,
        "gdp_growth_2030_high": gdp_high,
        "top3_vulnerabilities": vulnerabilities,
        "recommendations_short": recs
    })

with open(OUT_FILE, "w") as f:
    json.dump(results, f, indent=3, ensure_ascii=False)

print(f"Saved cleaned LLM policy brief at {OUT_FILE}")

!pip install -q pandas numpy scikit-learn pyarrow tqdm

import os
import re
import json
import warnings
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.preprocessing import FunctionTransformer

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

ARTIFACTS_DIR = "/content/drive/MyDrive/DPL 2025/artifacts"
INPUT_PARQUET = os.path.join(ARTIFACTS_DIR, "tren_master_fe.parquet")

OUT_BASELINE = os.path.join(ARTIFACTS_DIR, "tren_forecasts_2030_baseline.json")
OUT_SOCSPEND = os.path.join(ARTIFACTS_DIR, "tren_forecasts_2030_social_spending.json")
OUT_TRADEDIV = os.path.join(ARTIFACTS_DIR, "tren_forecasts_2030_trade_diversification.json")
OUT_CRISIS   = os.path.join(ARTIFACTS_DIR, "tren_forecasts_2030_global_crisis.json")
OUT_COMBINED = os.path.join(ARTIFACTS_DIR, "tren_forecasts_2030_all_scenarios.json")
OUT_CSV      = os.path.join(ARTIFACTS_DIR, "tren_forecasts_2030_all_scenarios.csv")


if not os.path.exists(INPUT_PARQUET):
    raise FileNotFoundError(f"Missing {INPUT_PARQUET}. Please ensure earlier pipeline saved 'tren_master_fe.parquet'.")

df = pd.read_parquet(INPUT_PARQUET)

required_cols = {"iso3", "country", "year"}
missing_req = required_cols - set(df.columns)
if missing_req:
    raise ValueError(f"Missing required columns in master file: {missing_req}")

def pick_poverty_column(cols: List[str]) -> Optional[str]:

    candidates = [
        r"poverty.*national.*(rate|pct|percent)",
        r"poverty.*3\.65.*(rate|pct|percent)",
        r"poverty.*2\.15.*(rate|pct|percent)",
        r"poverty.*(rate|pct|percent)",
        r"pct_poverty|poverty_rate|poverty_pct"
    ]
    low = [c.lower() for c in cols]
    for pat in candidates:
        for c in cols:
            if re.search(pat, c, flags=re.IGNORECASE):
                return c
    return None

def pick_trade_resilience_column(cols: List[str]) -> Optional[str]:

    if "resilience_score" in cols:
        return "resilience_score"
    return None

GDP_GROWTH_COL = "gdp_growth_pct" if "gdp_growth_pct" in df.columns else None
POVERTY_COL    = pick_poverty_column(df.columns.tolist())
TRADE_RES_COL  = pick_trade_resilience_column(df.columns.tolist())

print("Targets detected:")
print("  GDP growth:", GDP_GROWTH_COL or "❌ not found (skipping)")
print("  Poverty rate:", POVERTY_COL or "❌ not found (skipping)")
print("  Trade resilience:", TRADE_RES_COL or "❌ not found (will proxy if possible)")


ID_COLS = ["iso3", "country", "year"]

num_df = df.drop(columns=[c for c in df.columns if c not in ID_COLS and not pd.api.types.is_numeric_dtype(df[c])], errors="ignore")

if TRADE_RES_COL is None:
    proxy_terms = []
    invert_cols = []
    if "trade_partner_hhi" in num_df.columns:
        invert_cols.append("trade_partner_hhi")
    if "trade_dependency_index" in num_df.columns:
        invert_cols.append("trade_dependency_index")
    if "shock_impact_score" in num_df.columns:
        invert_cols.append("shock_impact_score")

    tmp = num_df.copy()
    for c in invert_cols:
        tmp[c] = -tmp[c]

    zcols = []
    for c in invert_cols:
        m = tmp[c].mean(skipna=True)
        s = tmp[c].std(skipna=True)
        if s is None or s == 0 or np.isnan(s):
            z = (tmp[c] - m)
        else:
            z = (tmp[c] - m) / s
        zcols.append(z)

    if zcols:
        proxy = pd.concat(zcols, axis=1).mean(axis=1)
        proxy_min, proxy_max = proxy.min(), proxy.max()
        tr_proxy = 100 * (proxy - proxy_min) / (proxy_max - proxy_min + 1e-9)
        num_df["trade_resilience_proxy"] = tr_proxy
        TRADE_RES_COL = "trade_resilience_proxy"
        print("  → Using trade_resilience_proxy as target.")
    else:
        print("  → Could not build trade resilience proxy. Scenario TR will be skipped.")

def winsorize_series(s: pd.Series, p_lo=0.01, p_hi=0.99) -> pd.Series:
    if s.dropna().empty:
        return s
    lo, hi = np.quantile(s.dropna(), [p_lo, p_hi])
    return s.clip(lower=lo, upper=hi)

def smooth_country_outliers(df: pd.DataFrame,
                            target_col: str,
                            special_caps: Dict[str, Tuple[float, float]] = None) -> pd.DataFrame:

    g = []
    for iso, sub in df.groupby("iso3", as_index=False):
        if target_col in sub.columns:
            sub[target_col] = winsorize_series(sub[target_col], 0.01, 0.99)
        g.append(sub)
    out = pd.concat(g, axis=0, ignore_index=True)

    special_caps = special_caps or {
        "PRT": (-5.0, 8.0),
        "SWE": (-5.0, 8.0),
        "HRV": (-6.0, 9.0),
    }
    if target_col in out.columns:
        for iso, (lo, hi) in special_caps.items():
            mask = out["iso3"].eq(iso)
            out.loc[mask, target_col] = out.loc[mask, target_col].clip(lo, hi)
    return out

if GDP_GROWTH_COL:
    num_df = smooth_country_outliers(num_df, GDP_GROWTH_COL)


def add_lags(panel: pd.DataFrame, cols: List[str], group="iso3", time="year", lags=[1,2]) -> pd.DataFrame:
    panel = panel.sort_values([group, time]).copy()
    for c in cols:
        if c not in panel.columns:
            continue
        for L in lags:
            panel[f"{c}_lag{L}"] = panel.groupby(group)[c].shift(L)
    return panel

lag_candidates = [
    GDP_GROWTH_COL, "inflation_consumer_prices_pct", "unemployment_total_pct",
    "trade_gdp_pct", "exports_goods_services_gdp_pct", "imports_goods_services_gdp_pct",
    "trade_partner_hhi", "trade_dependency_index", "shock_impact_score",
    "government_spending_pct_gdp", "gdp_per_capita_current_usd", "life_expectancy_total_years"
]
lag_cols = [c for c in lag_candidates if c in num_df.columns and c is not None]
panel = add_lags(num_df, lag_cols, lags=[1,2])


EXCLUDE = set(ID_COLS + [c for c in [GDP_GROWTH_COL, POVERTY_COL, TRADE_RES_COL] if c])
X_cols = [c for c in panel.columns if c not in EXCLUDE]

X_cols = [c for c in X_cols if pd.api.types.is_numeric_dtype(panel[c])]


def train_per_country_model(
    data: pd.DataFrame,
    target_col: str,
    min_year: int = 2004,
    max_year: int = 2024
):

    models = {}
    for iso, sub in data.groupby("iso3"):
        sub = sub[(sub["year"] >= min_year) & (sub["year"] <= max_year)].copy()
        if target_col not in sub.columns or sub[target_col].dropna().shape[0] < 8:
            continue

        X = sub[X_cols]
        y = sub[target_col]

        model = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("hgb", HistGradientBoostingRegressor(
                max_depth=6, learning_rate=0.08, max_iter=800, l2_regularization=0.02,
                random_state=42))
        ])
        try:
            model.fit(X, y)
            models[iso] = model
        except Exception as e:
            print(f"Skipping {iso} for target {target_col}: {e}")
    return models

def prepare_2030_frame(
    base_df: pd.DataFrame,
    year: int = 2030
) -> pd.DataFrame:

    latest = base_df.sort_values(["iso3", "year"]).groupby("iso3").tail(2).copy()
    last_by_iso = latest.groupby("iso3").tail(1).copy()
    last_by_iso["year"] = year

    def inject_lags(iso):
        sub = latest[latest["iso3"] == iso].sort_values("year")
        row2030 = last_by_iso[last_by_iso["iso3"] == iso].copy()
        for c in lag_cols:
            if c in sub.columns and c in row2030.columns:
                row2030[f"{c}_lag1"] = sub[c].iloc[-1] if len(sub) >= 1 else np.nan
                row2030[f"{c}_lag2"] = sub[c].iloc[-2] if len(sub) >= 2 else np.nan
        return row2030

    rows = []
    for iso in last_by_iso["iso3"].unique():
        rows.append(inject_lags(iso))
    f2030 = pd.concat(rows, axis=0, ignore_index=True)

    for c in X_cols:
        if c not in f2030.columns:
            f2030[c] = np.nan

    keep_cols = list(set(ID_COLS + X_cols))
    f2030 = f2030[keep_cols]
    return f2030


def scenario_transform(frame: pd.DataFrame, scenario: str) -> pd.DataFrame:

    dfc = frame.copy()

    def add_pp(col, pp):
        if col in dfc.columns:
            dfc[col] = dfc[col].astype(float) + pp

    def scale(col, factor):
        if col in dfc.columns:
            dfc[col] = dfc[col].astype(float) * factor

    if scenario == "baseline":
        return dfc

    if scenario == "social_spending":
        if "government_spending_pct_gdp" in dfc.columns:
            add_pp("government_spending_pct_gdp", 2.0)
        else:
            for c in [c for c in dfc.columns if "life_expectancy" in c]:
                scale(c, 1.01)
            for c in [c for c in dfc.columns if "poverty" in c]:
                scale(c, 0.98)

    elif scenario == "trade_diversification":
        if "trade_partner_hhi" in dfc.columns:
            scale("trade_partner_hhi", 0.85)
        if "trade_dependency_index" in dfc.columns:
            scale("trade_dependency_index", 0.90)
        if "exports_goods_services_gdp_pct" in dfc.columns:
            scale("exports_goods_services_gdp_pct", 1.05)

    elif scenario == "global_crisis":
        if "inflation_consumer_prices_pct" in dfc.columns:
            add_pp("inflation_consumer_prices_pct", 2.5)
        if "shock_impact_score" in dfc.columns:
            scale("shock_impact_score", 1.20)
        for c in [c for c in dfc.columns if c.startswith("gdp_growth_pct_lag")]:
            dfc[c] = dfc[c].astype(float) - 3.0
        if "exports_goods_services_gdp_pct" in dfc.columns:
            scale("exports_goods_services_gdp_pct", 0.97)
        if "imports_goods_services_gdp_pct" in dfc.columns:
            scale("imports_goods_services_gdp_pct", 0.97)

    return dfc

models_by_target = {}

if GDP_GROWTH_COL:
    models_by_target["gdp_growth"] = train_per_country_model(panel, GDP_GROWTH_COL)

if POVERTY_COL:
    models_by_target["poverty_rate"] = train_per_country_model(panel, POVERTY_COL)

if TRADE_RES_COL:
    models_by_target["trade_resilience"] = train_per_country_model(panel, TRADE_RES_COL)


f2030_base = prepare_2030_frame(panel, year=2030)

def predict_scenario(f2030: pd.DataFrame, scenario_name: str) -> pd.DataFrame:
    X_s = scenario_transform(f2030, scenario_name).copy()
    out_rows = []

    for iso, sub in X_s.groupby("iso3"):
        row = {
            "iso3": iso,
            "country": sub["country"].iloc[0] if "country" in sub.columns else iso,
            "scenario": scenario_name
        }
        Xi = sub[X_cols]


        if "gdp_growth" in models_by_target and iso in models_by_target["gdp_growth"]:
            pred = models_by_target["gdp_growth"][iso].predict(Xi)[0]
            row["gdp_growth_2030"] = float(np.clip(pred, -7.0, 10.0))
        else:
            row["gdp_growth_2030"] = None

        if "poverty_rate" in models_by_target and iso in models_by_target["poverty_rate"]:
            pred = models_by_target["poverty_rate"][iso].predict(Xi)[0]
            row["poverty_rate_2030"] = float(np.clip(pred, 0.0, 100.0))
        else:
            row["poverty_rate_2030"] = None

        if "trade_resilience" in models_by_target and iso in models_by_target["trade_resilience"]:
            pred = models_by_target["trade_resilience"][iso].predict(Xi)[0]
            row["trade_resilience_2030"] = float(np.clip(pred, 0.0, 100.0))
        else:
            row["trade_resilience_2030"] = None

        out_rows.append(row)

    return pd.DataFrame(out_rows)

baseline_df = predict_scenario(f2030_base, "baseline")
socspend_df = predict_scenario(f2030_base, "social_spending")
tradediv_df = predict_scenario(f2030_base, "trade_diversification")
crisis_df   = predict_scenario(f2030_base, "global_crisis")

def to_list_of_dicts(df: pd.DataFrame) -> List[Dict]:
    records = []
    for _, r in df.iterrows():
        records.append({
            "iso3": r["iso3"],
            "country": r["country"],
            "scenario": r["scenario"],
            "gdp_growth_2030": r.get("gdp_growth_2030", None),
            "poverty_rate_2030": r.get("poverty_rate_2030", None),
            "trade_resilience_2030": r.get("trade_resilience_2030", None),
        })
    return records

with open(OUT_BASELINE, "w") as f:
    json.dump(to_list_of_dicts(baseline_df), f, indent=2)

with open(OUT_SOCSPEND, "w") as f:
    json.dump(to_list_of_dicts(socspend_df), f, indent=2)

with open(OUT_TRADEDIV, "w") as f:
    json.dump(to_list_of_dicts(tradediv_df), f, indent=2)

with open(OUT_CRISIS, "w") as f:
    json.dump(to_list_of_dicts(crisis_df), f, indent=2)


combined = pd.concat([baseline_df, socspend_df, tradediv_df, crisis_df], ignore_index=True)
combined.to_csv(OUT_CSV, index=False)

nested = []
for iso, sub in combined.groupby("iso3"):
    entry = {
        "iso3": iso,
        "country": sub["country"].iloc[0],
        "scenarios": {}
    }
    for _, r in sub.iterrows():
        entry["scenarios"][r["scenario"]] = {
            "gdp_growth_2030": r["gdp_growth_2030"],
            "poverty_rate_2030": r["poverty_rate_2030"],
            "trade_resilience_2030": r["trade_resilience_2030"],
        }
    nested.append(entry)

with open(OUT_COMBINED, "w") as f:
    json.dump(nested, f, indent=2)

print("Saved files:")
print("  -", OUT_BASELINE)
print("  -", OUT_SOCSPEND)
print("  -", OUT_TRADEDIV)
print("  -", OUT_CRISIS)
print("  -", OUT_COMBINED)
print("  -", OUT_CSV)

import os, json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import geopandas as gpd

MASTER_FILE = "/content/drive/MyDrive/DPL 2025/artifacts/tren_master_fe.parquet"
SUMMARY_FILE = "/content/drive/MyDrive/DPL 2025/artifacts/policy_briefs_summary.json"
OUT_DIR = "/content/drive/MyDrive/DPL 2025/visuals"
os.makedirs(OUT_DIR, exist_ok=True)

master = pd.read_parquet(MASTER_FILE)
with open(SUMMARY_FILE, "r") as f:
    summary = json.load(f)
summary_df = pd.DataFrame(summary)

plt.figure(figsize=(12,6))
pivot = summary_df.pivot_table(index="country", values="gdp_growth_2030_med")
sns.heatmap(pivot, annot=True, cmap="coolwarm", cbar=True)
plt.title("2030 GDP Growth Forecast (Median %)")
plt.savefig(os.path.join(OUT_DIR, "heatmap_gdp.png"))
plt.close()

if "trade_dependency_index" in master.columns:
    td = master.groupby("iso3")["trade_dependency_index"].mean().dropna()
    G = nx.Graph()
    for iso, val in td.items():
        G.add_node(iso, weight=val)

    countries = td.index.tolist()
    for i in range(len(countries)):
        for j in range(i+1, len(countries)):
            diff = abs(td[countries[i]] - td[countries[j]])
            if diff < 5:
                G.add_edge(countries[i], countries[j])
    plt.figure(figsize=(10,8))
    nx.draw_networkx(G, with_labels=True, node_size=500, font_size=8)
    plt.title("Trade Dependency Similarities Network")
    plt.savefig(os.path.join(OUT_DIR, "trade_network.png"))
    plt.close()

if "shock_impact_score" in master.columns:
    if not os.path.exists("ne_countries/ne_110m_admin_0_countries.shp"):
        !wget -q https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip
        !unzip -o ne_110m_admin_0_countries.zip -d ne_countries

    world = gpd.read_file("ne_countries/ne_110m_admin_0_countries.shp")
    latest = master.sort_values("year").groupby("iso3").last().reset_index()
    merged = world.merge(latest, left_on="ISO_A3", right_on="iso3", how="left")

    fig, ax = plt.subplots(1, 1, figsize=(15,8))
    merged.plot(column="shock_impact_score", cmap="Reds", legend=True, ax=ax)
    plt.title("Shock Impact Score by Country")
    plt.savefig(os.path.join(OUT_DIR, "shock_map.png"))
    plt.close()


top_vuln = summary_df[["iso3","country","top3_vulnerabilities"]]
top_vuln.to_csv(os.path.join(OUT_DIR,"top3_vulnerabilities.csv"), index=False)

print(f"Visualizations saved in {OUT_DIR}")